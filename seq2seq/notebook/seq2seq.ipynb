{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "用seq2seq架构实现一个翻译模型，从中文翻译到英文，中文也用空格来分词进行简化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义语料\n",
    "\n",
    "使用参考书中的语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世界', '我', '强大', '非常', '复杂', '咖哥', '自然', '很', '深度学习', '喜欢', '学习', '人工智能', '改变', '语言', '处理', '小冰', '神经网络', '爱']\n",
      "['the', 'studying', 'world', 'NLP', 'KaGe', 'powerful', 'I', 'AI', 'complex', 'changed', 'is', 'likes', 'Neural-Nets', 'so', 'DL', 'XiaoBing', 'love', 'are', '<eos>', '<sos>']\n",
      "{'世界': 0, '我': 1, '强大': 2, '非常': 3, '复杂': 4, '咖哥': 5, '自然': 6, '很': 7, '深度学习': 8, '喜欢': 9, '学习': 10, '人工智能': 11, '改变': 12, '语言': 13, '处理': 14, '小冰': 15, '神经网络': 16, '爱': 17}\n",
      "{'the': 0, 'studying': 1, 'world': 2, 'NLP': 3, 'KaGe': 4, 'powerful': 5, 'I': 6, 'AI': 7, 'complex': 8, 'changed': 9, 'is': 10, 'likes': 11, 'Neural-Nets': 12, 'so': 13, 'DL': 14, 'XiaoBing': 15, 'love': 16, 'are': 17, '<eos>': 18, '<sos>': 19}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    ['咖哥 喜欢 小冰','<sos> KaGe likes XiaoBing','KaGe likes XiaoBing <eos>'],\n",
    "    ['我 爱 学习 人工智能','<sos> I love studying AI','I love studying AI <eos>'],\n",
    "    ['深度学习 改变 世界','<sos> DL changed the world','DL changed the world <eos>'],\n",
    "    ['自然 语言 处理 很 强大','<sos> NLP is so powerful','NLP is so powerful <eos>'],\n",
    "    ['神经网络 非常 复杂','<sos> Neural-Nets are complex','Neural-Nets are complex <eos>']\n",
    "]\n",
    "\n",
    "word_list_cn, word_list_en = [],[]\n",
    "\n",
    "for s in sentences:\n",
    "    word_list_cn.extend(s[0].split())\n",
    "    word_list_en.extend(s[1].split())\n",
    "    word_list_en.extend(s[2].split())\n",
    "\n",
    "word_list_cn = list(set(word_list_cn))\n",
    "word_list_en = list(set(word_list_en))\n",
    "\n",
    "word2idx_cn = {w:i for i,w in enumerate(word_list_cn)}\n",
    "word2idx_en = {w:i for i,w in enumerate(word_list_en)}\n",
    "\n",
    "voc_size_cn = len(word_list_cn)\n",
    "voc_size_en = len(word_list_en)\n",
    "\n",
    "print(word_list_cn)\n",
    "print(word_list_en)\n",
    "print(word2idx_cn)\n",
    "print(word2idx_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 17, 10, 11]])\n",
      "tensor([[19,  6, 16,  1,  7]])\n",
      "tensor([[ 6, 16,  1,  7, 18]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import torch\n",
    "import random\n",
    "\n",
    "def make_data(sentences):\n",
    "    random_sentence = random.choice(sentences)\n",
    "    encoder_input = np.array([[word2idx_cn[n] for n in random_sentence[0].split()]])\n",
    "    decoder_input = np.array([[word2idx_en[n] for n in random_sentence[1].split()]])\n",
    "    target = np.array([[word2idx_en[n] for n in random_sentence[2].split()]])\n",
    "\n",
    "    encoder_input = torch.LongTensor(encoder_input)\n",
    "    decoder_input = torch.LongTensor(decoder_input)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    return encoder_input,decoder_input,target\n",
    "\n",
    "encoder_input, decoder_input, target = make_data(sentences)\n",
    "\n",
    "print(encoder_input)\n",
    "print(decoder_input)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码器结构: Encoder(\n",
      "  (embedding): Embedding(18, 128)\n",
      "  (rnn): RNN(128, 128, batch_first=True)\n",
      ")\n",
      "解码器结构: Decoder(\n",
      "  (embedding): Embedding(20, 128)\n",
      "  (rnn): RNN(128, 128, batch_first=True)\n",
      "  (out): Linear(in_features=128, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "n_hidden = 128\n",
    "encoder = Encoder(voc_size_cn, n_hidden)\n",
    "decoder = Decoder(n_hidden, voc_size_en)\n",
    "\n",
    "print(\"编码器结构:\", encoder)\n",
    "print(\"解码器结构:\", decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
