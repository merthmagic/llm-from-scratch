{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先准备句子，进行分词，形成词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\n",
    "#     \"The New York Times is a daily newspaper based in New York City\",\n",
    "#     \"The effort failed once local California newspapers came into prominence\",\n",
    "#     \"Shortly after assuming control of the paper Ochs coined the paper slogan\"\n",
    "# ]\n",
    "# sentences = [\n",
    "#     \"Kage is Teacher\",\n",
    "#     \"Niuzong is Boss\",\n",
    "#     \"Mazong is Boss\",\n",
    "#     \"Xiaoxue is Student\",\n",
    "#     \"Xiaobing is Student\"\n",
    "# ]\n",
    "sentences = [\n",
    "    \"Cat is animal\",\"Dog is animal\",\"Lion is animal\",\"Merlin is bird\",\"Pidgin is bird\"\n",
    "]\n",
    "words = list(set((' '.join(sentences)).split()))\n",
    "print(f\"words :{words}\")\n",
    "print(f\"vocabular size:{len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "映射一个 word => index 的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word:idx for idx,word in enumerate(words)}\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`windows size` 是窗口大小，取目标词前后各 window size 个词作为上下文: `[max(idx-window_size,0):min(idx+window_size+1,len(splitted_sentence))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    splitted_sentence = sentence.split()\n",
    "    for idx,word in enumerate(splitted_sentence):\n",
    "        for neighbor in splitted_sentence[max(idx-window_size,0):min(idx+window_size+1,len(splitted_sentence))]:\n",
    "            if neighbor != word:\n",
    "                data.append((neighbor,word))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行独热编码，用`pytorch`中的`tensor`表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def one_hot_encoding(word,word_to_idx):\n",
    "    tensor = torch.zeros(len(word_to_idx))\n",
    "    tensor[word_to_idx[word]] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# 对data全体进行独热编码\n",
    "encoded_data = [(one_hot_encoding(context,word_to_idx),word_to_idx[target]) for context,target in data]\n",
    "print(encoded_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义神经网络准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self,voc_size,embedding_size):\n",
    "        super(SkipGram,self).__init__()\n",
    "        self.input_to_hidden = nn.Linear(voc_size,embedding_size,bias=False)\n",
    "        self.hidden_to_output = nn.Linear(embedding_size,voc_size,bias=False)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        hidden = self.input_to_hidden(X)\n",
    "        output = self.hidden_to_output(hidden)\n",
    "        return output\n",
    "\n",
    "embedding_size = 2\n",
    "skipgram_model = SkipGram(voc_size=len(words),embedding_size=embedding_size)\n",
    "print(\"Skip-Gram:\",skipgram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(skipgram_model.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_valus = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    for context,target in data:\n",
    "        X = one_hot_encoding(target,word_to_idx).float().unsqueeze(0)\n",
    "        y_true = torch.tensor([word_to_idx[context]],dtype=torch.long)\n",
    "        y_pred = skipgram_model(X)\n",
    "        loss = criterion(y_pred,y_true)\n",
    "        # 累计损失\n",
    "        loss_sum += loss.item()\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad() \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch:{epoch+1},Loss:{loss_sum/len(data)}\")\n",
    "        loss_valus.append(loss_sum/len(data))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.plot(range(1, epochs // 100 + 1), loss_valus)\n",
    "plt.title(\"训练损失曲线\")\n",
    "plt.xlabel(\"轮次\")\n",
    "plt.ylabel(\"损失\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取词嵌入信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,idx in word_to_idx.items():\n",
    "    print(f\"{word}:{skipgram_model.input_to_hidden.weight[:,idx].detach().numpy()}\")\n",
    "\n",
    "# print(\"\\nSkip-Gram词嵌入:\")\n",
    "# for word, idx in word_to_idx.items(): # 输出每个单词的嵌入向量\n",
    "#     print(f\"{word}: \\\n",
    "#     {skipgram_model.input_to_hidden.weight[:, idx].detach().numpy()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skipgram_model.input_to_hidden.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词嵌入图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for word,idx in word_to_idx.items():\n",
    "    vec = skipgram_model.input_to_hidden.weight[:,idx].detach().numpy()\n",
    "    ax.scatter(vec[0],vec[1])\n",
    "    ax.annotate(word,(vec[0],vec[1]),fontsize=9)\n",
    "plt.title('二维词嵌入')\n",
    "plt.xlabel('向量维度1')\n",
    "plt.ylabel('向量维度2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
