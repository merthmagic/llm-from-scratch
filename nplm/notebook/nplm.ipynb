{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPLM\n",
    "Neural Probabilistic Language Model\n",
    "\n",
    "神经概率语言模型\n",
    "\n",
    "<img src=\"attachments/nplm-structure.jpg\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建语料库\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'喜欢': 0, '玩具': 1, '我': 2, '讨厌': 3, '爸爸': 4, '挨打': 5, '爱': 6}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"我 喜欢 玩具\",\n",
    "    \"我 爱 爸爸\",\n",
    "    \"我 讨厌 挨打\"\n",
    "] # 这里的中文用了空格来简化分词lol\n",
    "\n",
    "word_list = list(set(' '.join(sentences).split()))\n",
    "word_to_idx = {word:idx for idx,word in enumerate(word_list)}\n",
    "voc_size = len(word_list)\n",
    "idx_to_word =  {idx:word for idx,word in enumerate(word_list)}\n",
    "\n",
    "print(word_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入批处理数据:tensor([[2, 6],\n",
      "        [2, 0]])\n",
      "输入批处理数据对应的原始词:[['我', '爱'], ['我', '喜欢']]\n",
      "目标批处理数据:tensor([4, 1])\n",
      "目标批处理数据对应的原始词:['爸爸', '玩具']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    selected_sentences = random.sample(sentences,batch_size)\n",
    "    \n",
    "    for sentence in selected_sentences:\n",
    "        words = sentence.split()\n",
    "        input = [word_to_idx[n] for n in words[:-1]] # 最后一个词以外的词作为输入\n",
    "        target = word_to_idx[words[-1]] # 最后一个词作为目标\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "    \n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    \n",
    "    return input_batch,target_batch\n",
    "\n",
    "input_batch,target_batch = make_batch()\n",
    "print(f\"输入批处理数据:{input_batch}\")\n",
    "\n",
    "\n",
    "input_words = []\n",
    "for input_idx in input_batch:\n",
    "    input_words.append([idx_to_word[idx.item()] for idx in input_idx])\n",
    "    # for idx in input_idx:\n",
    "    #     input_words.append([idx_to_word[idx.item()]])\n",
    "        \n",
    "print(f\"输入批处理数据对应的原始词:{input_words}\")\n",
    "print(f\"目标批处理数据:{target_batch}\")\n",
    "\n",
    "target_words = [idx_to_word[idx.item()] for idx in target_batch]\n",
    "print(f\"目标批处理数据对应的原始词:{target_words}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPLM(\n",
      "  (C): Embedding(7, 2)\n",
      "  (linear1): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_size = 2\n",
    "n_step = 2\n",
    "n_hidden = 2\n",
    "\n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NPLM,self).__init__()\n",
    "        self.C = nn.Embedding(voc_size,embedding_size)\n",
    "        self.linear1 = nn.Linear(n_step*embedding_size,n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden,voc_size)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.C(X)\n",
    "        X = X.view(-1,n_step*embedding_size)\n",
    "        hidden = torch.tanh(self.linear1(X))\n",
    "        output = self.linear2(hidden)\n",
    "        return output\n",
    "    \n",
    "nplm_model = NPLM()\n",
    "print(nplm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost= 0.246516\n",
      "Epoch: 2000 cost= 0.001458\n",
      "Epoch: 3000 cost= 0.000286\n",
      "Epoch: 4000 cost= 0.000135\n",
      "Epoch: 5000 cost= 0.000074\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nplm_model.parameters(),lr=0.1)\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    input_batch,target_batch = make_batch()\n",
    "    output = nplm_model(input_batch)\n",
    "    loss = criterion(output,target_batch)\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print('Epoch:','%4d' % (epoch+1),'cost=','{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '讨厌'] -> 挨打\n",
      "['我', '喜欢'] -> 玩具\n"
     ]
    }
   ],
   "source": [
    "input_strs = [['我','讨厌'],['我','喜欢']]\n",
    "input_indices = [[word_to_idx[word] for word in seq]for seq in input_strs]\n",
    "input_batch = torch.LongTensor(input_indices)\n",
    "predict = nplm_model(input_batch).data.max(1)[1]\n",
    "predict_strs = [idx_to_word[n.item()]for n in predict.squeeze()]\n",
    "for input_seq,pred in zip(input_strs,predict_strs):\n",
    "    print(input_seq,'->',pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
