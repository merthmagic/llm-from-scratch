{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阅读材料\n",
    "\n",
    "[Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 笔记\n",
    "\n",
    "1. A single time step of the input is supplied to the network i.e. xt is supplied to the network\n",
    "2. We then calculate its current state using a combination of the current input and the previous state i.e. we calculate ht\n",
    "3. The current ht becomes ht-1 for the next time step\n",
    "4. We can go as many time steps as the problem demands and combine the information from all the previous states\n",
    "5. Once all the time steps are completed the final current state is used to calculate the output yt\n",
    "6. The output is then compared to the actual output and the error is generated\n",
    "7. The error is then backpropagated to the network to update the weights(we shall go into the details of backpropagation in further sections) and the network is trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单实现\n",
    "\n",
    "样本用'hello'这个单词，目标是输入'hell',能推理出'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letters:['h', 'e', 'l', 'l', 'o']\n",
      "vocabular:['h', 'e', 'o', 'l']\n",
      "voc size:4\n"
     ]
    }
   ],
   "source": [
    "word = 'hello'\n",
    "letters = list(word)\n",
    "print(f\"letters:{letters}\")\n",
    "words = list(set(letters))\n",
    "voc_size = len(words)\n",
    "print(f\"vocabular:{words}\")\n",
    "print(f\"voc size:{voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 1, 'e': 2, 'l': 3, 'o': 4}\n"
     ]
    }
   ],
   "source": [
    "# 保持和文章一致方便对照，这里硬编码word to idx\n",
    "word_to_idx = {\n",
    "    'h':0,\n",
    "    'e':1,\n",
    "    'l':2,\n",
    "    'o':3\n",
    "}\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     tensor[word_to_idx[word]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m----> 8\u001b[0m encoded_words \u001b[38;5;241m=\u001b[39m [(one_hot_encoding(word),word)\u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_words)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m     tensor[word_to_idx[word]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m----> 8\u001b[0m encoded_words \u001b[38;5;241m=\u001b[39m [(\u001b[43mone_hot_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m,word)\u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_words)\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mone_hot_encoding\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot_encoding\u001b[39m(word):\n\u001b[0;32m      4\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(word_to_idx))\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def one_hot_encoding(word):\n",
    "    tensor = torch.zeros(len(word_to_idx))\n",
    "    tensor[word_to_idx[word]] = 1\n",
    "    return tensor\n",
    "\n",
    "encoded_words = [(one_hot_encoding(word),word)for word in words]\n",
    "print(encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = torch.tensor([\n",
    "    [0.287027,0.84606,0.572392,0.486813],\n",
    "    [0.902874,0.871522,0.691079,0.18998],\n",
    "    [0.537524,0.09224,0.558159,0.491528]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "print(Wxh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = torch.tensor([1,0,0,0],dtype=torch.float32)\n",
    "print(Xt)\n",
    "print(Xt.shape)\n",
    "# print(Xt.view(-1,1))\n",
    "\n",
    "result = torch.matmul(Wxh,Xt)\n",
    "\n",
    "print(result)\n",
    "print(result.shape)\n",
    "result1 = torch.matmul(Wxh,Xt.view(-1,1))\n",
    "print(result1)\n",
    "print(result1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索一下矩阵和行向量，gpt生成了一个例子\n",
    "\n",
    "# 定义一个1x3的矩阵\n",
    "matrix = torch.tensor([[1, 2, 3]])\n",
    "\n",
    "# 定义一个dim为3的行向量\n",
    "row_vector = torch.tensor([1, 2, 3])\n",
    "\n",
    "print(matrix)\n",
    "print(row_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在这个示例中，我们定义了一个1x3的矩阵matrix和一个dim为3的行向量row_vector，它们在大多数情况下可以等价使用。\n",
    "\n",
    "> 然而，在一些情况下，例如矩阵乘法等操作中，PyTorch可能会对它们进行不同的处理，因此在进行特定操作时可能需要注意它们的类型和维度。但在大多数情况下，这两种表示都可以等价使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机初始化测试\n",
    "\n",
    "rnd = torch.rand(1,1)\n",
    "\n",
    "print(rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机初始变量\n",
    "Whh = torch.rand(1,1)\n",
    "bias = torch.rand(1,1)\n",
    "\n",
    "print(Whh)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一下计算\n",
    "\n",
    "# state = torch.matmul(Whh,result1).add(bias)\n",
    "\n",
    "print(Whh * result1 + bias)\n",
    "#print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tanh(Whh * result1 + bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对原文的`Why`有疑惑，还有计算的完整过程也有没理解的地方，用文中数据实际进行计算来反推\n",
    "\n",
    "进行完整的过程计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whh = torch.tensor([[0.427043]])\n",
    "bias = torch.tensor([[0.57700]])\n",
    "\n",
    "print(f\"Whh={Whh},bias={bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步计算时，前一个状态是没有的，所以用[0,0,0]\n",
    "\n",
    "h0 = torch.tensor([[0],[0],[0]])\n",
    "print(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(previous_state):\n",
    "    return Whh * previous_state + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1,计算 'h'\n",
    "calced_h0 = calc(h0)\n",
    "print(calced_h0)\n",
    "\n",
    "Wxh_Xt = torch.matmul(Wxh,one_hot_encoding('h').view(-1,1))\n",
    "print(Wxh_Xt)\n",
    "\n",
    "print(Wxh)\n",
    "\n",
    "# h1 = torch.tanh( calc(h0)+ torch.matmul(Wxh,one_hot_encoding('h').view(-1,1)))\n",
    "# print(h1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2，计算 'e'\n",
    "h2 = torch.tanh( calc(h1)+ torch.matmul(Wxh,one_hot_encoding('e').view(-1,1)))\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3，计算 'l'\n",
    "h3 = torch.tanh( calc(h2)+ torch.matmul(Wxh,one_hot_encoding('l').view(-1,1)))\n",
    "print(h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4，计算 'l'\n",
    "h4 = torch.tanh( calc(h3)+ torch.matmul(Wxh,one_hot_encoding('l').view(-1,1)))\n",
    "print(h4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
