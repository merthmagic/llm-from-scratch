{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER\n",
    "\n",
    "阅读材料:\n",
    "\n",
    "https://download.csdn.net/blog/column/10841550/129149699"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Middlewares\\anaconda3\\envs\\NLP2\\lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for peoples_daily_ner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/peoples_daily_ner\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "dataset = load_dataset('peoples_daily_ner',token=token)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='hfl/rbt6', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt6\")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305,  680, 7032,\n",
       "         7305,  722, 7313, 4638, 3862, 1818,  511,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 6821, 2429,  898, 2255,  988, 3717, 4638, 1300, 4289, 7667, 4507,\n",
       "         1744, 1079,  671, 3837, 4638, 6392, 6369, 2360,  712, 2898, 6392, 6369,\n",
       "         8024, 3146,  702, 2456, 5029, 5408, 5125, 5401, 5445, 2612, 2131,  511,\n",
       "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(\n",
    "    [\n",
    "        [\n",
    "            \"海\",\n",
    "            \"钓\",\n",
    "            \"比\",\n",
    "            \"赛\",\n",
    "            \"地\",\n",
    "            \"点\",\n",
    "            \"在\",\n",
    "            \"厦\",\n",
    "            \"门\",\n",
    "            \"与\",\n",
    "            \"金\",\n",
    "            \"门\",\n",
    "            \"之\",\n",
    "            \"间\",\n",
    "            \"的\",\n",
    "            \"海\",\n",
    "            \"域\",\n",
    "            \"。\",\n",
    "        ],\n",
    "        [\n",
    "            \"这\",\n",
    "            \"座\",\n",
    "            \"依\",\n",
    "            \"山\",\n",
    "            \"傍\",\n",
    "            \"水\",\n",
    "            \"的\",\n",
    "            \"博\",\n",
    "            \"物\",\n",
    "            \"馆\",\n",
    "            \"由\",\n",
    "            \"国\",\n",
    "            \"内\",\n",
    "            \"一\",\n",
    "            \"流\",\n",
    "            \"的\",\n",
    "            \"设\",\n",
    "            \"计\",\n",
    "            \"师\",\n",
    "            \"主\",\n",
    "            \"持\",\n",
    "            \"设\",\n",
    "            \"计\",\n",
    "            \"，\",\n",
    "            \"整\",\n",
    "            \"个\",\n",
    "            \"建\",\n",
    "            \"筑\",\n",
    "            \"群\",\n",
    "            \"精\",\n",
    "            \"美\",\n",
    "            \"而\",\n",
    "            \"恢\",\n",
    "            \"宏\",\n",
    "            \"。\",\n",
    "        ],\n",
    "    ],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    is_split_into_words=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20852,\n",
       " ['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        #names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "        # 如果实体是个3字人名，应该表现为 122；\n",
    "\n",
    "        #在线加载数据集\n",
    "        dataset = load_dataset(path='peoples_daily_ner', split=split)\n",
    "        \n",
    "        #离线加载数据集\n",
    "#         data_path = '/Users/xx/Downloads/NER_in_Chinese-main/data'\n",
    "        #dataset = load_from_disk(dataset_path=data_path)[split]\n",
    "\n",
    "        #过滤掉太长的句子\n",
    "        def f(data):\n",
    "            return len(data['tokens']) <= 512 - 2\n",
    "\n",
    "        dataset = dataset.filter(f)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tokens = self.dataset[i]['tokens']\n",
    "        labels = self.dataset[i]['ner_tags']\n",
    "\n",
    "        return tokens, labels\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "tokens, labels = dataset[0]\n",
    "\n",
    "# 查看数据样例\n",
    "# 有两万多条数据，\n",
    "len(dataset), tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据整理函数\n",
    "def collate_fn(data):\n",
    "    tokens = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True)\n",
    "    \n",
    "\n",
    "    # 找到最长的句子 长度\n",
    "    lens = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # 对句子进行补充，头部/尾部+7，然后截取到最长的长度； 7不存在于\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [7] + labels[i]\n",
    "        labels[i] += [7] * lens\n",
    "        labels[i] = labels[i][:lens]\n",
    "\n",
    "    return inputs, torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n",
      "[CLS] 至 少 在 1 5 米 以 外 的 、 来 自 尼 日 利 亚 的 裁 判 鲍 查 多 略 犹 豫 一 下 ， 鸣 笛 判 罚 点 球 ， 从 而 救 了 意 大 利 队 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 1, 2, 2, 2,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 0, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
      "input_ids torch.Size([16, 85])\n",
      "token_type_ids torch.Size([16, 85])\n",
      "attention_mask torch.Size([16, 85])\n"
     ]
    }
   ],
   "source": [
    "# 查看数据样例\n",
    "for i, (inputs, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader)) # \n",
    "print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "print(labels[0])\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5974.0416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 85, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = AutoModel.from_pretrained('hfl/rbt6')\n",
    "\n",
    "pretrained = pretrained.to(device)\n",
    "\n",
    "# 统计参数量 6kw \n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000)\n",
    "\n",
    "# 模型试算\n",
    "# [b, lens] -> [b, lens, 768]\n",
    "inputs = inputs.to(device)\n",
    "pretrained(**inputs).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 85, 8])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None  # 默认情况下，让预训练模型 并不属于下游任务模型的一部分；\n",
    "        \n",
    "        # 这里定义下游模型只有简单两层网络 RNN(GRU)+全连接\n",
    "        self.rnn = torch.nn.GRU(768, 768,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(768, 8)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        if self.tuneing:  # 如果是 tuneing模式，预训练模型属于自己模型的一部分，使用自己的预训练模型进行计算 \n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():  # 使用外部的预训练模型进行计算\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc(out).softmax(dim=2)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # 切换模型是否 tuneing 的模式 \n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True  # 预训练模型也进行参数更新，计算参数梯度 \n",
    "\n",
    "            pretrained.train() # 预训练模型也是训练模式 \n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval() # 运行模式，不梯度更新\n",
    "            self.pretrained = None\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model(inputs).shape # torch.Size([16, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4650, -0.7309,  0.0480, -1.2878, -0.4159,  0.3020,  1.3637,  0.2327],\n",
       "         [ 1.2281,  0.6102, -0.6525,  1.0780,  1.7088,  0.6285,  0.0045,  0.9218],\n",
       "         [-0.6620, -0.0954, -0.5760,  0.3274,  0.1450, -1.5258,  1.6959,  1.1467],\n",
       "         [ 0.0093, -0.3787,  0.3565, -0.3704, -1.1949, -0.4401,  0.9939,  1.5427],\n",
       "         [-0.8264,  0.8301, -0.6192,  1.0794,  0.8094, -1.8064, -1.0796, -1.5944],\n",
       "         [ 0.0753,  0.8013, -0.1799,  0.3314,  0.4346,  1.5185, -1.2893, -0.4233]]),\n",
       " tensor([1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1、对计算结果和label变形,并且移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    #变形,便于计算loss\n",
    "    #[b, lens, 8] -> [b*lens, 8]\n",
    "    outs = outs.reshape(-1, 8)\n",
    "    #[b, lens] -> [b*lens]\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    #[b, lens] -> [b*lens - pad]\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2, 3, 8), torch.ones(2, 3),\n",
    "                       torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 16, 3, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_correct_and_total_count(labels, outs):\n",
    "    #[b*lens, 8] -> [b*lens]\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    #计算除了0以外元素的正确率,因为0太多了,包括的话,正确率很容易虚高\n",
    "    select = labels != 0\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "model_path='./model/ner.model'\n",
    " \n",
    "def train(epochs):\n",
    "    lr = 2e-5 if model.tuneing else 5e-4\n",
    "\n",
    "    #训练\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    #optimizer = optimizer.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #模型计算\n",
    "            #[b, lens] -> [b, lens, 8]\n",
    "            outs = model(inputs)\n",
    "            outs = outs.to(device)\n",
    "            \n",
    "\n",
    "            #对outs和label变形,并且移除pad\n",
    "            #outs -> [b, lens, 8] -> [c, 8]\n",
    "            #labels -> [b, lens] -> [c]\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                                  inputs['attention_mask'])\n",
    "            \n",
    "            outs,labels = outs.to(device),labels.to(device)\n",
    "\n",
    "            #梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 50 == 0: # 计算正确数\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "                accuracy = counts[0] / counts[1] \n",
    "                accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "                print('-- ', epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "        \n",
    "        \n",
    "        torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: rnn.weight_ih_l0, Device: cuda:0\n",
      "Parameter: rnn.weight_hh_l0, Device: cuda:0\n",
      "Parameter: rnn.bias_ih_l0, Device: cuda:0\n",
      "Parameter: rnn.bias_hh_l0, Device: cuda:0\n",
      "Parameter: fc.weight, Device: cuda:0\n",
      "Parameter: fc.bias, Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(False)\n",
    "model = model.to(device)\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  0 0 1.3471088409423828 0.9269005847953217 0.3902439024390244\n",
      "--  0 50 1.3740086555480957 0.9 0.3018867924528302\n",
      "--  0 100 1.3917919397354126 0.8822170900692841 0.23880597014925373\n",
      "--  0 150 1.334036111831665 0.9399727148703957 0.42105263157894735\n",
      "--  0 200 1.363770604133606 0.9102384291725105 0.3333333333333333\n",
      "--  0 250 1.380600929260254 0.8934081346423562 0.2962962962962963\n",
      "--  0 300 1.395739197731018 0.8782696177062375 0.20915032679738563\n",
      "--  0 350 1.325000524520874 0.9490084985835694 0.47058823529411764\n",
      "--  0 400 1.3658952713012695 0.9081145584725537 0.29357798165137616\n",
      "--  0 450 1.3706334829330444 0.9033760186263097 0.2782608695652174\n",
      "--  0 500 1.3781756162643433 0.8958333333333334 0.26229508196721313\n",
      "--  0 550 1.37593412399292 0.8980747451868629 0.26229508196721313\n",
      "--  0 600 1.394051432609558 0.8799582463465553 0.21768707482993196\n",
      "--  0 650 1.4532809257507324 0.8207282913165266 0.2\n",
      "--  0 700 1.4029980897903442 0.8710106382978723 0.24806201550387597\n",
      "--  0 750 1.3861582279205322 0.8878504672897196 0.27586206896551724\n",
      "--  0 800 1.3983402252197266 0.8756684491978609 0.256\n",
      "--  0 850 1.3786921501159668 0.8953168044077136 0.2962962962962963\n",
      "--  0 900 1.3563623428344727 0.9176470588235294 0.3368421052631579\n",
      "--  0 950 1.3858026266098022 0.8882063882063882 0.2601626016260163\n",
      "--  0 1000 1.3622040748596191 0.9118046132971506 0.32989690721649484\n",
      "--  0 1050 1.3916560411453247 0.8823529411764706 0.2962962962962963\n",
      "--  0 1100 1.457971215248108 0.8160377358490566 0.21476510067114093\n",
      "--  0 1150 1.3968391418457031 0.87716955941255 0.25806451612903225\n",
      "--  0 1200 1.3617281913757324 0.9122807017543859 0.3137254901960784\n",
      "--  0 1250 1.3886433839797974 0.8853658536585366 0.25396825396825395\n",
      "--  0 1300 1.3947991132736206 0.8792102206736353 0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "# 非 fine-tuneing 训练一个轮次\n",
    "\n",
    "model = model.to(device)\n",
    "model.fine_tuneing(False)\n",
    "#print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(1)  # 354 万个参数\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6329.012\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "--  0 0 2.100078582763672 0.022222222222222223 0.14583333333333334\n",
      "--  0 50 1.4523428678512573 0.8191823899371069 0.0\n",
      "--  0 100 1.4256821870803833 0.8511904761904762 0.04580152671755725\n",
      "--  0 150 1.355615496635437 0.9195011337868481 0.1839080459770115\n",
      "--  0 200 1.3985035419464111 0.8777777777777778 0.29357798165137616\n",
      "--  0 250 1.369827389717102 0.9050847457627119 0.27586206896551724\n",
      "--  0 300 1.3590667247772217 0.9157754010695187 0.3368421052631579\n",
      "--  0 350 1.395281195640564 0.8792884371029225 0.25196850393700787\n",
      "--  0 400 1.3657935857772827 0.9086134453781513 0.2689075630252101\n",
      "--  0 450 1.3636161088943481 0.9106967615309126 0.2601626016260163\n",
      "--  0 500 1.4681082963943481 0.8062015503875969 0.20382165605095542\n",
      "--  0 550 1.358036994934082 0.9162371134020618 0.32989690721649484\n",
      "--  0 600 1.4195319414138794 0.8546895640686922 0.22535211267605634\n",
      "--  0 650 1.3461920022964478 0.9280359820089955 0.4\n",
      "--  0 700 1.4998488426208496 0.7743300423131171 0.16666666666666666\n",
      "--  0 750 1.36143958568573 0.9127625201938611 0.37209302325581395\n",
      "--  0 800 1.4472460746765137 0.8268974700399467 0.19753086419753085\n",
      "--  0 850 1.3854695558547974 0.8886532343584306 0.23357664233576642\n",
      "--  0 900 1.3441050052642822 0.9300254452926209 0.367816091954023\n",
      "--  0 950 1.3850587606430054 0.8890784982935154 0.32989690721649484\n",
      "--  0 1000 1.383805751800537 0.890295358649789 0.2909090909090909\n",
      "--  0 1050 1.4074394702911377 0.8666666666666667 0.26666666666666666\n",
      "--  0 1100 1.3772375583648682 0.8968609865470852 0.31683168316831684\n",
      "--  0 1150 1.4516905546188354 0.8223844282238443 0.1797752808988764\n",
      "--  0 1200 1.3519421815872192 0.9221556886227545 0.38095238095238093\n",
      "--  0 1250 1.348055362701416 0.9260273972602739 0.37209302325581395\n",
      "--  0 1300 1.3824286460876465 0.891644908616188 0.2782608695652174\n",
      "--  1 0 1.3587524890899658 0.9153225806451613 0.3368421052631579\n",
      "--  1 50 1.41714346408844 0.8569321533923304 0.24806201550387597\n",
      "--  1 100 1.373841404914856 0.9002217294900222 0.26229508196721313\n",
      "--  1 150 1.4422980546951294 0.8317631224764468 0.20382165605095542\n",
      "--  1 200 1.4011465311050415 0.8729139922978177 0.24427480916030533\n",
      "--  1 250 1.3993184566497803 0.8747346072186837 0.21333333333333335\n",
      "--  1 300 1.3728491067886353 0.9012048192771084 0.2807017543859649\n",
      "--  1 350 1.3176466226577759 0.9564068692206077 0.484375\n",
      "--  1 400 1.3845559358596802 0.8894952251023193 0.2831858407079646\n",
      "--  1 450 1.3765268325805664 0.8975155279503105 0.24427480916030533\n",
      "--  1 500 1.4050538539886475 0.8689903846153846 0.22695035460992907\n",
      "--  1 550 1.371147632598877 0.9028985507246376 0.32323232323232326\n",
      "--  1 600 1.3932733535766602 0.8807692307692307 0.256\n",
      "--  1 650 1.356615424156189 0.9174434087882823 0.3404255319148936\n",
      "--  1 700 1.421887993812561 0.8521617852161785 0.2318840579710145\n",
      "--  1 750 1.3887754678726196 0.8852672750977836 0.26666666666666666\n",
      "--  1 800 1.4022117853164673 0.8718291054739653 0.25\n",
      "--  1 850 1.36963951587677 0.9043956043956044 0.2689075630252101\n",
      "--  1 900 1.3921815156936646 0.8818565400843882 0.27586206896551724\n",
      "--  1 950 1.3342468738555908 0.9397905759162304 0.41025641025641024\n",
      "--  1 1000 1.3358969688415527 0.9381443298969072 0.47058823529411764\n",
      "--  1 1050 1.4034792184829712 0.8705547652916074 0.2601626016260163\n",
      "--  1 1100 1.3224934339523315 0.9515418502202643 0.49230769230769234\n",
      "--  1 1150 1.362136960029602 0.9118942731277533 0.34782608695652173\n",
      "--  1 1200 1.3646332025527954 0.9093959731543624 0.2831858407079646\n",
      "--  1 1250 1.3632729053497314 0.9107551487414187 0.2909090909090909\n",
      "--  1 1300 1.3601959943771362 0.9138321995464853 0.2962962962962963\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "print(model.pretrained)\n",
    "train(2) # 6000多万个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28mprint\u001b[39m(s)\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m==========================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m():\n\u001b[1;32m----> 3\u001b[0m     model_load \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mmodel_path\u001b[49m)\n\u001b[0;32m      4\u001b[0m     model_load\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      6\u001b[0m     loader_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39mDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      7\u001b[0m                                               batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                               collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[0;32m      9\u001b[0m                                               shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                                               drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_path='./model/ner.model'\n",
    "def predict():\n",
    "    model_load = torch.load(model_path)\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(32):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "        label = labels[i, select]\n",
    "        \n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''))\n",
    "\n",
    "        #输出tag\n",
    "        for tag in [label, out]:\n",
    "            s = ''\n",
    "            for j in range(len(tag)):\n",
    "                if tag[j] == 0:\n",
    "                    s += '·'\n",
    "                    continue\n",
    "                s += tokenizer.decode(input_id[j])\n",
    "                s += str(tag[j].item())\n",
    "\n",
    "            print(s)\n",
    "        print('==========================')\n",
    "\n",
    "\n",
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
